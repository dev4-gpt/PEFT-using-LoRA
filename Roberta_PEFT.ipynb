{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Starter Notebook\n",
        "##Install and import required libraries"
      ],
      "metadata": {
        "id": "gUKQxppyNt6X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQUnjrzHMa-P"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n",
        "!pip install nvidia-ml-py3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pickle"
      ],
      "metadata": {
        "id": "5AmQBhZeMjOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Tokenizer and Preprocess Data"
      ],
      "metadata": {
        "id": "T_Tns4YvN0-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "BASE_MODEL = \"roberta-base\"\n",
        "DATASET_NAME = \"ag_news\"\n",
        "OUTPUT_DIR = \"roberta_lora_agnews_results\"\n",
        "MAX_LENGTH = 128\n",
        "RANDOM_SEED = 37\n",
        "\n",
        "# Load Dataset\n",
        "dataset = load_dataset(DATASET_NAME, split='train')\n",
        "print(f\"Loaded {len(dataset)} training samples.\")\n",
        "\n",
        "# Load Tokenizer\n",
        "print(f\"Loading tokenizer for base model: {BASE_MODEL}\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess(examples):\n",
        "    \"\"\"Tokenizes the text data with consistent parameters.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors=None # for batch processing\n",
        "    )\n",
        "\n",
        "# Apply Preprocessing\n",
        "tokenized_dataset = dataset.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\"]\n",
        "    )\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Explicit label mapping ensures consistency between training and inference\n",
        "id2label = {\n",
        "    0: \"World\",\n",
        "    1: \"Sports\",\n",
        "    2: \"Business\",\n",
        "    3: \"Sci/Tech\"\n",
        "}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "print(\"Explicit label mapping for model:\")\n",
        "print(f\"id2label: {id2label}\")\n",
        "print(f\"label2id: {label2id}\")"
      ],
      "metadata": {
        "id": "i7uYhjjzMni_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the number of classess and their names\n",
        "num_labels = dataset.features['label'].num_classes\n",
        "class_names = dataset.features[\"label\"].names\n",
        "print(f\"number of labels: {num_labels}\")\n",
        "print(f\"the labels: {class_names}\")\n",
        "\n",
        "# Create an id2label mapping\n",
        "# We will need this for our classifier.\n",
        "id2label = {i: label for i, label in enumerate(class_names)}\n",
        "label2id = {label: i for i, label in enumerate(class_names)}\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "yKfYb0JjMplD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Pre-trained Model\n",
        "##Set up config for pretrained model and download it from hugging face"
      ],
      "metadata": {
        "id": "Z_tQHdwdN4uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    num_labels=4,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id)\n",
        "model"
      ],
      "metadata": {
        "id": "x_mqMRDiMrJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Anything from here on can be modified"
      ],
      "metadata": {
        "id": "tKphlsGBN9Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Dataset\n",
        "split_datasets = tokenized_dataset.train_test_split(\n",
        "    test_size=0.1,\n",
        "    seed=37,\n",
        "    stratify_by_column=\"labels\"\n",
        "    )\n",
        "train_dataset = split_datasets[\"train\"]\n",
        "eval_dataset = split_datasets[\"test\"]\n",
        "print(f\"Training set size: {len(train_dataset)}\")\n",
        "print(f\"Evaluation set size: {len(eval_dataset)}\")"
      ],
      "metadata": {
        "id": "rBaEqQVwMsvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all parameters of the base model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "kjkCdOr2MuCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup LoRA Config\n",
        "##Setup PEFT config and get peft model for finetuning"
      ],
      "metadata": {
        "id": "2y_R81sEOAX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running PEFT Config on LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=8, # Rank of the update matrices, higher it is, better the capacity\n",
        "    lora_alpha=16, # LoRA scaling factor = 2*Rank\n",
        "    lora_dropout=0.2, # Dropout probability for LoRA layers\n",
        "    bias = 'lora_only',\n",
        "    target_modules = [\"query\", \"value\"],\n",
        "    task_type=\"SEQ_CLS\",\n",
        ")"
      ],
      "metadata": {
        "id": "f7yxbA9NMvVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Applying LoRA configuration to the base model-\")\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model"
      ],
      "metadata": {
        "id": "-ee6A_a9MwtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('PEFT Model')\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "1oBR6RvfMx4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate trainable parameters manually for verification\n",
        "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.4f}%\")\n",
        "\n",
        "# Verify we're under 1M trainable parameters\n",
        "assert trainable_params < 1_000_000, \"Trainable parameters exceed 1 million!\""
      ],
      "metadata": {
        "id": "TpIZjYgQMzHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Trainable parameters:\")\n",
        "for name, param in peft_model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "         print(name)"
      ],
      "metadata": {
        "id": "e6vjPzICM1FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Setup"
      ],
      "metadata": {
        "id": "cUINFGfgOGBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "\n",
        "# To track evaluation accuracy during training\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}"
      ],
      "metadata": {
        "id": "NdfwY_-6M2Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Training args\n",
        "output_dir = \"results\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"roberta-lora-agnews-results\",\n",
        "    # learning_rate=5e-4,\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=6,\n",
        "    # max_steps=1200 # consider either max steps or desired epochs\n",
        "    eval_strategy='steps', # Evaluate during training\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\", # Save checkpoints during training\n",
        "    save_steps=200, # Save checkpoints at the same frequency as evaluation\n",
        "    load_best_model_at_end=True, # Load the best model found during training\n",
        "    metric_for_best_model=\"accuracy\", # Use accuracy to determine the best model\n",
        "    greater_is_better=True,\n",
        "    weight_decay=0.01,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=1.0,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    report_to=None, # Disable default reporting integrations like wandb/tensorboard unless configured\n",
        "    fp16=torch.cuda.is_available(), # Use mixed precision if GPU is available\n",
        "    gradient_accumulation_steps=4, # Accumulate gradients to simulate larger batch size if needed\n",
        "    warmup_ratio=0.1, # Add a learning rate warmup phase\n",
        "    seed=37,\n",
        "    logging_steps=50,\n",
        "    gradient_checkpointing=True, # Enable if memory is constrained, might slow down training\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Arguments:\")\n",
        "print(training_args)"
      ],
      "metadata": {
        "id": "BkTeykyCM4Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize trainer\n",
        "def get_trainer(model):\n",
        "      return  Trainer(\n",
        "          model=peft_model,\n",
        "          args=training_args,\n",
        "          compute_metrics=compute_metrics,\n",
        "          train_dataset=train_dataset,\n",
        "          eval_dataset=eval_dataset,\n",
        "          data_collator=data_collator,\n",
        "          tokenizer=tokenizer,\n",
        "      )"
      ],
      "metadata": {
        "id": "gFtdwulaM5dI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Start Training"
      ],
      "metadata": {
        "id": "wSJYXGdsOKCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_lora_finetuning_trainer = get_trainer(peft_model)\n",
        "\n",
        "result = peft_lora_finetuning_trainer.train()"
      ],
      "metadata": {
        "id": "eWyXUw1yM6uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save Final Model\n",
        "print(\"Training finished. Saving model...\")\n",
        "peft_lora_finetuning_trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
        "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))"
      ],
      "metadata": {
        "id": "VBuRQD61M72c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "eval_metrics = peft_lora_finetuning_trainer.evaluate()\n",
        "print(f\"Final evaluation metrics: {eval_metrics}\")"
      ],
      "metadata": {
        "id": "l04Ciz1NM87N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(model, tokenizer, text):\n",
        "    \"\"\"Run inference on a single text example.\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "    prediction = output.logits.argmax(dim=-1).item()\n",
        "    print(f'Class: {prediction}, Label: {id2label[prediction]}, Text: {text}')\n",
        "    return id2label[prediction]"
      ],
      "metadata": {
        "id": "-qjWvJVhM-en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run Inference on eval_dataset"
      ],
      "metadata": {
        "id": "3sBKdUSTOoZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Inference on Test Examples\n",
        "test_examples = [\n",
        "    \"Wall Street rallies as tech stocks surge to new heights\",\n",
        "    \"Manchester United wins dramatic match against Liverpool\",\n",
        "    \"New study reveals breakthrough in cancer treatment\",\n",
        "    \"Tech company announces innovative AI product\",\n",
        "    \"NVIDIA owns AI\"\n",
        "]\n",
        "\n",
        "print(\"\\nRunning inference on test examples:\")\n",
        "for text in test_examples:\n",
        "    result = classify(peft_model, tokenizer, text)"
      ],
      "metadata": {
        "id": "7mGzQr0CM_yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(inference_model, dataset, labelled=True, batch_size=32, data_collator=None):\n",
        "    \"\"\"\n",
        "    Evaluate a PEFT model on a dataset.\n",
        "\n",
        "    Args:\n",
        "        inference_model: The model to evaluate.\n",
        "        dataset: The dataset (Hugging Face Dataset) to run inference on.\n",
        "        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n",
        "                         If False, only predictions will be returned.\n",
        "        batch_size (int): Batch size for inference.\n",
        "        data_collator: Function to collate batches. If None, the default collate_fn is used.\n",
        "\n",
        "    Returns:\n",
        "        If labelled is True, returns a tuple (metrics, predictions)\n",
        "        If labelled is False, returns the predictions.\n",
        "    \"\"\"\n",
        "    # Create the DataLoader\n",
        "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    inference_model.to(device)\n",
        "    inference_model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # Loop over the DataLoader\n",
        "      for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "          # Move each tensor in the batch to the device\n",
        "          batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = inference_model(**batch)\n",
        "          predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "          all_predictions.extend(predictions.cpu().numpy())\n",
        "          if labelled and \"labels\" in batch:\n",
        "              all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "              # Calculate accuracy\n",
        "              metric = evaluate.load('accuracy')\n",
        "\n",
        "    # Compute metrics if dataset is labelled\n",
        "    if labelled and all_labels:\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy,\n",
        "        }\n",
        "\n",
        "        print(f\"Evaluation metrics: {metrics}\")\n",
        "        return metrics, all_predictions\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "HsoCa2KQNEi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process Test Data for Submission\n",
        "try:\n",
        "    print(\"\\nLoading test data for Kaggle submission...\")\n",
        "    test_data_path = \"test_unlabelled.pkl\"\n",
        "\n",
        "    # Load the pickle file\n",
        "    test_df = pd.read_pickle(test_data_path)\n",
        "    print(f\"Loaded data type: {type(test_df)}\")\n",
        "\n",
        "    # If it's already a Dataset object, convert to DataFrame first\n",
        "    if hasattr(test_df, 'to_pandas'):\n",
        "        print(\"Converting Dataset to DataFrame...\")\n",
        "        test_df = test_df.to_pandas()\n",
        "    elif not isinstance(test_df, pd.DataFrame):\n",
        "        print(f\"Unexpected data type: {type(test_df)}. Converting to DataFrame...\")\n",
        "        if hasattr(test_df, 'features'):\n",
        "            # It's a Dataset-like object\n",
        "            test_df = pd.DataFrame({col: test_df[col] for col in test_df.column_names})\n",
        "        else:\n",
        "            # Try to convert directly\n",
        "            test_df = pd.DataFrame(test_df)\n",
        "\n",
        "    print(f\"Successfully loaded test data with shape: {test_df.shape}\")\n",
        "\n",
        "    # Ensure 'ID' column exists in the DataFrame\n",
        "    if \"ID\" not in test_df.columns:\n",
        "        test_df[\"ID\"] = range(len(test_df))\n",
        "\n",
        "    # Convert to Hugging Face Dataset\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Apply the same preprocessing as training data\n",
        "    test_dataset = test_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "    # Run inference\n",
        "    print(\"Running inference on test data...\")\n",
        "    predictions = peft_lora_finetuning_trainer.predict(test_dataset)\n",
        "    pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "    # Create submission DataFrame with EXACT column names required\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': test_df[\"ID\"],  # Use the original IDs from the test set!\n",
        "        'Label': pred_labels  # Numeric labels (0, 1, 2, 3)\n",
        "    })\n",
        "\n",
        "    # Save to CSV without index\n",
        "    submission_path = os.path.join(OUTPUT_DIR, \"kaggle_submission.csv\")\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Kaggle submission file saved to {submission_path}\")\n",
        "    print(submission_df.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error processing test data: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "2sGcpATVNGoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Base Model: {BASE_MODEL}\")\n",
        "print(f\"LoRA Rank (r): {peft_config.r}\")\n",
        "print(f\"LoRA Alpha: {peft_config.lora_alpha}\")\n",
        "print(f\"Target Modules: {peft_config.target_modules}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "print(f\"Final Evaluation Accuracy: {eval_metrics['eval_accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "A0ZdZarSNI2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "def extract_metrics_from_logs(output_dir):\n",
        "    log_file = os.path.join(output_dir, \"trainer_state.json\")\n",
        "    if not os.path.exists(log_file):\n",
        "        print(f\"Log file not found at {log_file}\")\n",
        "        return None\n",
        "\n",
        "    with open(log_file, 'r') as f:\n",
        "        logs = json.load(f)\n",
        "\n",
        "    metrics_dict = {\n",
        "        'train_loss': [],\n",
        "        'eval_loss': [],\n",
        "        'eval_accuracy': [],\n",
        "        'steps': []\n",
        "    }\n",
        "\n",
        "    for log in logs.get('log_history', []):\n",
        "        if 'loss' in log and 'step' in log and 'eval_loss' not in log:\n",
        "            metrics_dict['train_loss'].append(log['loss'])\n",
        "            metrics_dict['steps'].append(log['step'])\n",
        "\n",
        "        if 'eval_loss' in log and 'eval_accuracy' in log:\n",
        "            metrics_dict['eval_loss'].append(log['eval_loss'])\n",
        "            metrics_dict['eval_accuracy'].append(log['eval_accuracy'])\n",
        "\n",
        "    return metrics_dict\n",
        "\n",
        "# 1. Plot training and evaluation metrics over time\n",
        "def plot_training_metrics(metrics_dict):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(metrics_dict['steps'][:len(metrics_dict['train_loss'])],\n",
        "             metrics_dict['train_loss'], 'b-', label='Training Loss')\n",
        "\n",
        "    # Create positions for eval metrics\n",
        "    eval_steps = np.linspace(0, max(metrics_dict['steps']), len(metrics_dict['eval_loss']))\n",
        "    plt.plot(eval_steps, metrics_dict['eval_loss'], 'r-', label='Evaluation Loss')\n",
        "\n",
        "    plt.title('Loss vs Training Steps')\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot evaluation accuracy\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(eval_steps, metrics_dict['eval_accuracy'], 'g-', label='Evaluation Accuracy')\n",
        "    plt.title('Accuracy vs Training Steps')\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0.8, 1.0)  # Assuming accuracy is between 0.8 and 1.0, adjust as needed\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_metrics.png', dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bW642B8iNKeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = extract_metrics_from_logs(\"roberta-lora-agnews-results/checkpoint-800/\")\n",
        "plot_training_metrics(metrics)"
      ],
      "metadata": {
        "id": "oeGF85mTNL2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0RN4CQxXNVA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}